= CDC Synchronization of PostgreSQL and Elasticsearch with Spring Boot

*Motivation*

Some colleagues  of mine  use a database and _Elasticsearch_(_ELK_) together where the database is the main data source, and _ELK_ is the search facade: data is first stored in the database, and then the changes are written to the index.

Here is a typical example of such code that uses _Spring Data_:

    @Override
    @Transactional
    public ProductDto update(ProductDto dto) {
        var jpaEntity = jpaRepository.save(dto2jpa(Objects.requireNonNull(dto)));
        var elkEntity = elkRepository.save(jpa2elk(jpaEntity));
        return elk2dto(elkEntity);
    }

Unfortunately, this approach does not guarantee the consistency of the database and the index, not only permanently, but also with a delay. Namely, there may be such a moment that the state of the database at that moment will not correspond to the state of the index at any moment. And vice versa. In particular, some changes in the database may never reach the index.

Since I've seen code similar to the above in several projects, I thought a little project showing how sync could be done would be helpful.

This is an example of such a _Java_ project using _Postgresql_ _CDC_(Change Data Capture), _Elasticsearch_, _String Boot_, _String Data_(_JPA_, _Elasticsearch_). The project does not use any external components such as _Kafka_, etc. Only the ones listed. There are ten classes in total.

*Configuration*


In order to see how the project works, you must first set up the database.

Open some client (_psql_, _DBeaver_, etc) and run the command

`SHOW wal_level;`

If the value is not "_logical_", then you need to execute the command

`ALTER SYSTEM SET wal_level = logical;`

Then you need to restart the database server or a container with it (at least for _PostgreSQL 14_).

After that, open the client, look at the available slots

`SELECT * FROM pg_replication_slots;`

and create a slot with a unique name

``SELECT * FROM pg_create_logical_replication_slot('elk_slot', 'test_decoding', false, true);
``

The unique name of the slot (in the example '_elk_slot_') can be chosen by yourself.

Let's leave the plugin name ('_test_decoding_') as it is for now.

The rest of the settings can be left unchanged.

In order to return to the previous configuration, you must delete the created slot

`SELECT * FROM pg_drop_replication_slot ( 'elk_slot' );`

restore the previous value of _wal_level_

`ALTER SYSTEM RESET  wal_level;`

and restart the server or the container again.


*Project description*


To make the project compact, but still fully functional, I limited myself to working with one entity - _Product_, which corresponds to the table of the same name in the database (see script _resources/create_table.sql_), the index of the same name and three Java classes: _ProductDto_, _JPA_ entity _ProductDB_ and _ELK_ document _ProductELK_.

Two repositories are used to work with the entity

`ProductJPARepository extends JpaRepository<ProductDB, Long>`

``ProductELKRepository extends ElasticsearchRepository<ProductELK,Long>
``

The _ProductVanillaService_ service works with these repositories. This service does not care about database and index synchronization. The code from the Motivation section is taken from this service.

The controller that works with the service is _ProductController_. Only _CRUD_ endpoints are implemented for brevity.

If the application is launched with the "_vanilla_" profile, then it will work without synchronization.

In order to enable synchronization, you need to run the application with the “_sync_” profile.

To keep the database and the index in sync _ProductCDCService_ service is used instead of _ProductVanillaService_. The sync version uses the same repositories as the vanilla one.

Here is an example of processing data changes from ProductCDCService, similar to the example given in the Motivation section.

    @Override
    @Transactional(propagation= Propagation.NEVER)
    public ProductDto update(ProductDto dto) {
        var id = Objects.requireNonNull(Objects.requireNonNull(dto).getId());
        jpaRepository.save(dto2jpa(dto));
        processNextCDCChunk();
        return elkRepository.findById(id).map(ProductMapper::elk2dto).orElse(null);
    }

The both  services work with the database in the same way. Except that the methods of the vanilla service are transactional and therefore saving data to the index occurs in the same transaction as saving data to the database.

The methods of the _ProductCDCService_ service are not transactional.

Also, the _ProductCDCService_ methods, unlike _ProductVanillaService_, do not store data in the index.
Instead, it calls _processNextCDCChunk()_ method, which starts scanning completed transactions from _WAL_ and finaly upload changes in the index.

_processNextCDCChunk()_ starts its work outside a transaction. The transaction is opened in

`dbRepository.save(dto2db(dto))`

and closed  on  exit from it.

Therefore, by the time _processNextCDCChunk()_ is called, the data saved in the previous step is already in _WAL_ and available for scanning by this method. And, as a result, by the time _processNextCDCChunk()_ ends, this data will already be in the index.

With the next call of _elkRepository.findById(id)_ we get this data from the index and return it to the controller.

Here the next question arises. Does the state of the returned object match the state the object was at call of _update(dto)_? For example, if user A changed the product name to "_prodA_", will the name stay that way in the output?
The answer to this question is negative. The correct answer is as follows.
We return the state of the product at SOME point in time after the database transaction was committed.

Let's take an example.
Suppose user A renamed the product to "_prodA_" and user B followed him to "_prodB_".
If by the time _processNextCDCChunk()_ method of user A  completed  _WAL_ scan, the transaction with user B's change
has been committed, then the result of this commit will be included in the scan of user A, and user A's _elkRepository.findById(id)_ will return the product with the value of the field name "_prodB_".

There is nothing strange in this, as it is clear that the state of the database is saved in the index with some delay. The main thing is that all database changes reach the index.

In particular, after successfully saving changes to the database and to the index, we may return nothing to the user. This can happen if, following the changes that we process in client A's transaction, this object is deleted in client B's transaction. And if by the time user A scans  _WAL_, the deletion is already committed, then user A's _processNextCDCChunk()_ will remove the object from the index. In this case, upon return from _processNextCDCChunk()_, this object will no longer be in the index. Unless, of course, some third user C re-createed the deleted product and this product has uploaded into the index by the time _elkRepository.findById(id)_ of user A is called. In this case, we will return the product of user C to the user A, not the product of user A with the name "_prodA_".



The _processNextCDCChunk()_ method is simple. It calls the _TestDecodingCDCService.processNextCDCChunk()_ method and handles exceptions. It is important to note two points here.

The first is that the method is called asynchronously.

And the second is that the executor that executes the method uses one thread. This ensures that _WAL_ is processed sequentially, and therefore data changes are loaded into the index in exactly the same order as those changes were loaded into the database. Therefore, the state of the index at each moment will correspond to the state of the database at some moment in the past.


_TestDecodingCDCService.processNextCDCChunk()_ is the central method. It takes all committed unprocessed transactions, parses and uploads changes from scanned transactions  into the index.

Uploading changes to the index  (_TransactionOperationProcessor.processOp()_) consists of the following steps:

1. Based on the table name from the transaction operation (_TransactionOperation.tableName_), determine the _JPA_ entity class that is persisted in this table. If the class is not defined, then the operation is skipped.

2. Based on the _JPA_ class, find the _ELK_ service that processes the entities of this class. This service wraps  _ProductELKRepository_.  _ProductVanillaService_ uses this repository to upload _JPA_ entity into the index.  If no such service is found, then the operation is skipped.

3. Using _JPA_, restore the _JPA_ entity by _TransactionOperation.restoreSQLStatement_. Since the operation is associated with a specific table (_TransactionOperation.tableName_), only the part of the entity that is persisted in that table is restored. _JPA_ properties annotated with _@OneToOne_, _@OneToMany_, etc. are not initialized.

4. Upload the restored _JPA_  entity into the index with the service found in step 2.


If there are any problems during the processing of operations, you need to fix the problems and re-run _TestDecodingCDCService.processNextCDCChunk()_.
This can be done because the _ProductELKRepository.save_ and _ProductELKRepository.delete_ are idempotent.

If all scanned transactions are successfully processed, then the corresponding records are removed from _WAL_.

I hope provided information is helpful.

Please contact me if you have any comments, suggestions or questions.

Stay in sync,

Sergey






